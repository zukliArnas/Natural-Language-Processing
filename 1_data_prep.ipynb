{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd3834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "# %pip install librosa\n",
    "import librosa\n",
    "import kagglehub\n",
    "from typing import Dict, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a42105",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_WORDS = [\"bird\", \"eight\", \"four\", \"happy\", \"left\", \"marvin\", \"one\", \"seven\", \"three\", \"zero\"]\n",
    "DATA_DIR = \"data\"\n",
    "FILES_PER_WORD = 100\n",
    "KAGGLE_DATASET_REF = \"neehakurelli/google-speech-commands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5594c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_dataset_path() -> Optional[str]:\n",
    "    \"\"\"Downloads the dataset from Kaggle Hub.\"\"\"\n",
    "    source_path: str = \"\"\n",
    "    print(f'Downloading dataset from Kaggle: {KAGGLE_DATASET_REF}..')\n",
    "    try:\n",
    "        source_path = kagglehub.dataset_download(KAGGLE_DATASET_REF)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset from Kaggle: {e}\")\n",
    "        return None\n",
    "    return str(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "263bd645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from Kaggle: neehakurelli/google-speech-commands..\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.9), please consider upgrading to the latest version (0.3.13).\n"
     ]
    }
   ],
   "source": [
    "path = get_full_dataset_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2144be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(path: str) -> int:\n",
    "    \"\"\"Counts the number of word folders.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    \n",
    "    word_folders = 0\n",
    "    for entry in os.listdir(path):\n",
    "        full_path = os.path.join(path, entry)\n",
    "        if os.path.isdir(full_path) and entry != \"_background_noise_\":\n",
    "            word_folders += 1\n",
    "            \n",
    "    return word_folders\n",
    "\n",
    "def get_speakers_per_word(path: str) -> Dict[str, int]:\n",
    "    \"\"\"Counts unique speakers based on file naming convention (speaker_id_...).\"\"\"\n",
    "    speakers_map: Dict[str, int] = {}\n",
    "    if not os.path.exists(path):\n",
    "        return speakers_map\n",
    "\n",
    "    word_folders = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d)) and d != \"_background_noise_\"]\n",
    "\n",
    "    for word in sorted(word_folders):\n",
    "        word_path = os.path.join(path, word)\n",
    "        unique_speakers: set[str] = set()\n",
    "        for file_path in glob.glob(os.path.join(word_path, '*.wav')):\n",
    "            filename = os.path.basename(file_path)\n",
    "            # Speaker ID is usually the part before the first underscore\n",
    "            speaker_id = filename.split('_')[0] \n",
    "            unique_speakers.add(speaker_id)\n",
    "        \n",
    "        speakers_map[word] = len(unique_speakers)\n",
    "        \n",
    "    return speakers_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9858c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_copy_random_files(source_dir: str, target_dir: str, words: List[str], num_files: int = FILES_PER_WORD) -> None:\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n3. Selecting {num_files} random files per word and copying to ./{target_dir}...\")\n",
    "    \n",
    "    for word in words:\n",
    "        source_word_path = os.path.join(source_dir, word)\n",
    "        target_word_path = os.path.join(target_dir, word)\n",
    "        \n",
    "        if not os.path.isdir(source_word_path):\n",
    "            print(f\"\\n[Warning] Source folder not found for '{word}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        all_files = glob.glob(os.path.join(source_word_path, '*.wav'))\n",
    "        \n",
    "        files_to_select = random.sample(all_files, num_files)\n",
    "        \n",
    "        os.makedirs(target_word_path, exist_ok=True)\n",
    "        \n",
    "        for file_path in files_to_select:\n",
    "            shutil.copy(file_path, target_word_path)\n",
    "\n",
    "    print(f\"\\nDataset subset creation complete. Total {len(words) * num_files} files copied.\")\n",
    "\n",
    "def check_sampling_rate(data_dir: str, words_list: List[str]) -> Optional[float]:\n",
    "    \"\"\"Checks the sampling rate of a sample file in the processed directory.\"\"\"\n",
    "    first_file_path = None\n",
    "    \n",
    "    for word in words_list:\n",
    "        word_path = os.path.join(data_dir, word)\n",
    "        files = glob.glob(os.path.join(word_path, '*.wav'))\n",
    "        if files:\n",
    "            first_file_path = files[0]\n",
    "            break\n",
    "            \n",
    "    if first_file_path:\n",
    "        try:\n",
    "            _, sr = librosa.load(first_file_path, sr=None) \n",
    "            return sr\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking sampling rate: {e}\")\n",
    "            return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cb9b701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from Kaggle: neehakurelli/google-speech-commands..\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.9), please consider upgrading to the latest version (0.3.13).\n",
      "\n",
      "--- 1. FULL DATASET STATISTICS (Google Speech Commands) ---\n",
      "Total distinct word folders: 30\n",
      "Average unique speakers per word: 1186.8\n",
      "\n",
      "3. Selecting 100 random files per word and copying to ./data...\n",
      "\n",
      "Dataset subset creation complete. Total 1000 files copied.\n",
      "\n",
      "Background noise files copied to data/_background_noise_\n",
      "\n",
      "--- 4. FINAL SUBSET DOCUMENTATION ---\n",
      "\n",
      "Sampling Rate (SR): 16000 Hz (used for audio files)\n",
      "Words Selected: ['bird', 'eight', 'four', 'happy', 'left', 'marvin', 'one', 'seven', 'three', 'zero']\n",
      "Utterances per word: 100\n"
     ]
    }
   ],
   "source": [
    "source_dir = get_full_dataset_path()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    if source_dir:\n",
    "        print(\"\\n--- 1. FULL DATASET STATISTICS (Google Speech Commands) ---\")\n",
    "        \n",
    "        total_words = get_word_count(source_dir)\n",
    "        print(f\"Total distinct word folders: {total_words}\")\n",
    "        \n",
    "        speaker_counts = get_speakers_per_word(source_dir)\n",
    "        if speaker_counts:\n",
    "            avg_speakers = sum(speaker_counts.values()) / len(speaker_counts) if speaker_counts else 0\n",
    "            print(f\"Average unique speakers per word: {avg_speakers:.1f}\")\n",
    "        \n",
    "        # Select and copy the subset\n",
    "        select_and_copy_random_files(source_dir, DATA_DIR, LIST_OF_WORDS, FILES_PER_WORD)\n",
    "\n",
    "        # Copy background noise files\n",
    "        background_noise_source = os.path.join(source_dir, \"_background_noise_\")\n",
    "        background_noise_target = os.path.join(DATA_DIR, \"_background_noise_\")\n",
    "        \n",
    "        if os.path.exists(background_noise_source):\n",
    "            shutil.copytree(background_noise_source, background_noise_target, dirs_exist_ok=True)\n",
    "            print(f\"\\nBackground noise files copied to {background_noise_target}\")\n",
    "        else:\n",
    "            print(\"\\n[Warning] No background noise folder found in the source dataset.\")\n",
    "\n",
    "        # Verification and report documentation\n",
    "        print(\"\\n--- 4. FINAL SUBSET DOCUMENTATION ---\")        \n",
    "        sampling_rate = check_sampling_rate(DATA_DIR, LIST_OF_WORDS)\n",
    "        print(f\"\\nSampling Rate (SR): {sampling_rate} Hz (used for audio files)\")\n",
    "        print(f\"Words Selected: {LIST_OF_WORDS}\")\n",
    "        print(f\"Utterances per word: {FILES_PER_WORD}\")\n",
    "    else:\n",
    "        print(\"\\nStopping script due to dataset acquisition failure.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
